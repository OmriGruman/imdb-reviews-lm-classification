{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Classification\n",
    "### Language Model vs End-to-End"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews(dir_path, max_read=None):\n",
    "    review_files = os.listdir(dir_path)\n",
    "    review_files = random.sample(review_files, max_read) if max_read else review_files\n",
    "    print(f\"Read {len(review_files)} reviews from {dir_path}\")\n",
    "\n",
    "    reviews = []\n",
    "    for review_file in review_files:\n",
    "        with open(os.path.join(dir_path, review_file)) as f:\n",
    "            reviews.append(f.read())\n",
    "\n",
    "    return reviews, [int(re.findall(r'_(\\d+)', filename)[0]) for filename in review_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = r\"aclImdb\\train\"\n",
    "TEST_PATH = r\"aclImdb\\test\"\n",
    "NEGATIVE_DIR = \"neg\"\n",
    "POSITIVE_DIR = \"pos\"\n",
    "UNLABELED_DIR = \"unsup\"\n",
    "\n",
    "SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10 reviews from aclImdb\\train\\pos\n",
      "Read 10 reviews from aclImdb\\train\\neg\n",
      "Read 10 reviews from aclImdb\\train\\unsup\n",
      "Read 10 reviews from aclImdb\\test\\pos\n",
      "Read 10 reviews from aclImdb\\test\\neg\n"
     ]
    }
   ],
   "source": [
    "train_pos_text, train_pos_rating = read_reviews(os.path.join(TRAIN_PATH, POSITIVE_DIR), SAMPLE_SIZE)\n",
    "train_neg_text, train_neg_rating = read_reviews(os.path.join(TRAIN_PATH, NEGATIVE_DIR), SAMPLE_SIZE)\n",
    "train_unlabeled_text, _ = read_reviews(os.path.join(TRAIN_PATH, UNLABELED_DIR), SAMPLE_SIZE)\n",
    "test_pos_text, test_pos_rating = read_reviews(os.path.join(TEST_PATH, POSITIVE_DIR), SAMPLE_SIZE)\n",
    "test_neg_text, test_neg_rating = read_reviews(os.path.join(TEST_PATH, NEGATIVE_DIR), SAMPLE_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Language Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Exploration & Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for line break tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- SMALL SPOILER HEREIN! - <br /><br />When I looked at the votes for \"Creep\" today, I was surprised about so many IMDb-users rating this movie \"1\". I am wondering: what do people expect of such kind of movie? Are there so many people watching a movie without knowing anything about it?<br /><br />\"Creep\" is a HORROR movie and it is a pretty good one! This automatically means: it has a absurd story full of holes, outrageous hazards and simple one-side-characters. So why complain about it? Just take some popcorn and coke, make yourself comfortable in your seat and then... ...enjoy to be scared to death! The first 60 minutes when there is almost nothing else than Kate and a lonely subway station are incredibly scaring. There is suspense and fear in every corner of the screen and you will give some jerks just because of a sudden sound of a blinking neon lamp in the back of you. (In my opinion the sound editor did the best job in this movie.)<br /><br />When Kate meet her pursuer the quality of the movie drops but it still doesn't become a bad movie. The second part of the movie is not scary at all, but the gore effects are well done and the story is quite well developed. (...as long as you keep in mind that it is a horror movie you are watching and not the Discovery Channel!) So, if you want to make yourself a very, very scaring hour. Just watch the first hour of \"Creep\" and then leave the cinema or turn of your TV. You know the rest of the story anyway, don't you? The bad guy will die and Kate will be the only survivor. But even if you watch this movie up to its end, you will not be disappointed.<br /><br />...and if you want to push anxiety to the next level, do what I did after watching this movie: leave the cinema by dark night, go to the next subway station and take the last train home...\n",
      "This was on at 2 or so In the morning one Saturday a few years ago, for various reasons I don't remember the entire story but what remains are the two standout performances from the central characters. Dom has had a unfortunate lot, manipulated & literally working a rubbish job, Eugene torn between personal aspirations and duty towards his sibling. Tom Hulce' Dom doesn't plead for sympathy - It comes naturally. Ray Liotta Is a universe away from Henry Hill, displaying a soft centre In what must feel a thankless position.<br /><br />In many ways this deals with the dilemma many young carer's face - the past or the future. As It turns out, with some work the two can happily co-exist. Thoughtfully handled & sensitively played Dominick & Eugene Is difficult not to warm to.\n",
      "<br /><br />Arriving by boxcar in New York City, the shrewd young woman with the BABY FACE begins to methodically canoodle her way to the top floors of power in a great bank.<br /><br />Barbara Stanwyck is fascinating as the amoral heroine of this influential pre-Code drama. Without a shred of decency or regret, she coolly manipulates the removal or destruction of the men unlucky enough to find themselves in her way. A wonderful actress, Stanwyck has full opportunity here to display her ample talents.<br /><br />Appearing quite late in the story, George Brent is a welcome addition as the one fellow possibly able to handle Stanwyck; his sophisticated style of acting makes a nice counterpoint to her icy demeanor. Douglas Dumbrille, Donald Cook & Henry Kolker portray a succession of her unfortunate victims.<br /><br />John Wayne appears for just a few scant seconds as an unsuccessful suitor for Stanwyck's affections. This would be the only time these two performers appeared together on screen.<br /><br />Movie mavens should recognize Nat Pendleton as a speakeasy customer, and Charles Sellon & Edward Van Sloan as bank executives - all unbilled.<br /><br />The music heard on the soundtrack throughout the film, perfectly punctuating the plot, is Baby Face' (1926) by Benny Davis & Harry Akst and St. Louis Blues' (1914) by W.C. Handy.<br /><br />BABY FACE is a prime example of pre-Code naughtiness. In its frank & unapologetic dealing with sex, it is precisely the kind of film which the implementation of the Production Code in 1934 was meant to eliminate.\n",
      "Even longtime Shirley fans may be surprised by \"Now and Forever.\" The movie was filmed with Paramount studios  not with Shirley's parent company Twentieth Century Fox  in 1934, before Fox producer Darryl Zanuck had perfected the successful Shirley formula (cute songs, cold hearts for her to melt, young couples for her to play cupid to, happy endings). Thus \"Now and Forever\" falls into the category of a Shirley vehicle without the standard Shirley story. It is an awkward position for any movie, but this impressive, talented cast makes it work.<br /><br />Gary Cooper and Carole Lombard star as fun-loving, irresponsible con artists Jerry and Toni Day. The only thing that this devoted yet dysfunctional duo seems to hate more than being together is being apart. When they are suddenly landed with custody of Jerry's young daughter Penny (Shirley Temple), it is Toni  and not Penny, as many believe  who persuades Jerry to give up his criminal career. But Jerry flounders at his desk job, and desperate to prove that he can provide for his new family, he soon returns to thieving and dishonesty. In a standard Shirley device, Penny tries to melt the heart of crusty curmudgeon Felix Evans, the victim of one of Jerry's cons, but her attempt fails, for Evans is revealed to be a con artist himself, and he blackmails Jerry into helping him steal jewels. The drama, gunfight, death, and sorrow that follow all make this film a very unusual one for Little Miss Sunshine. There is no happy ending, no dancing, and only one song sequence (the cute number \"The World Owes Me a Living\").<br /><br />But this does not mean that Shirley fans should avoid \"Now and Forever.\" Rather, it's divergence from the usual Shirley story make it more interesting and memorable than many of her other films. But beware: You should avoid colorized version of this film, and see it in black-and-white if you can. The color is bright, garish, and unrealistic, and in many scenes, Shirley's famous curls are actually red instead of blonde. Yikes!\n",
      "It's hard to say which comes out on top, James Cagney's charm and energy or the mouth- opening excesses of Busby Berkeley's three grand showstoppers at the close. I give it a tie, with Footlight Parade one of the funniest and quickest of the early Thirties musicals. Although the movie clearly belongs to Cagney, Joan Blondell adds immeasurably to the good-natured story line. <br /><br />And what's the story line? It's about Chester Kent (Cagney) who produces musicals, and who now is just about out of business as the talkies take over. He starts doing Prologues, live musical entertainment offered on stage before a movie starts. He gets the idea to do bigger ones and more of them, moving them around the country. He's a ball of fire and ideas, and he needs all the ideas he can get to keep relentlessly producing these things. But a rival is spying on him and stealing his ideas; Nan Prescott (Blondell), his wise-cracking secretary, loves him but he's too busy too notice; an office girl in black-rimmed, round glasses (Ruby Keeler) wants a chance to dance; his wife turns up saying she didn't divorce him after all; a blonde gold-digger is setting her hooks in him; his partners are cheating him...my gosh, what's next? This may all sound like a lot to digest, but everything happens fast, with Cagney bouncing, strutting, striding, finger-snapping, barking orders and occasionally - until the big last number when he goes all out singing and dancing -- doing a step or two just to show how it's done. <br /><br />Instead of \"Let's put on a show, gang\" we have \"We need to build three shows in three days, so lock the doors and let's start rehearsing.\" These three super Prologues are going to feature 40 chorines, spectacular effects and will mean a rich contract, with forty Kent units in deluxe movie houses...the whole Apollo movie house circuit! Exhaustion threatens, feet ache, but all those unbilled chorines in skimpy costumes (which include Ann Sothern and Dorothy Lamour; you can quickly spot Sothern but Lamour is more generic) stay the course, dancing their hearts out, giggling and chattering and looking remarkably unsweaty. <br /><br />And then the curtains go up as each Prologue is presented in separate movie houses, one after the other on the same night, with the owner of the Apollo circuit going to determine that night whether he'll save Chester's skin or not. <br /><br />First up is \"Honeymoon Hotel\" with Dick Powell and Ruby Keeler in a 9 minute production number that features a lot of wholesome lasciviousness, with brides and grooms (some might even be married), bedrooms and beds, and doors with \"Do not disturb\" signs. <br /><br />Then on to the next theater and 11 minutes of \"By a Waterfall\" that probably had the Warner Brothers accountants worrying about bankruptcy. This number is so excessive -- dozens of swimming girls, trees, fountains, a huge grotto with waterslides, a giant pool -- you'd never think there was a Depression on. Berkeley pulls out all his tricks -- synchronization, human patterns, legs and arms doing all sorts of precision things -- and he does it in the water, with a lot of underwater photography looking up. The girls are sure game. They come up smiling with water in their eyes and still hit their marks. The whole thing must have been incredibly difficult and exhausting. Ruby Keeler, who has a couple of quick shots in the water, is the only one who looks a bit cautious. <br /><br />And finally, the smash finale...11 minutes of Cagney dancing and singing with Keeler to \"Shanghai Lil,\" with all sorts of bar girls and their customers, unusual in that the races are mixed up. There's Cagney and Keeler dancing on the bar, dancing on a table, Cagney fighting. There are what looks like fifty or sixty marching marines, hupping back and forth, rifles tossed and caught. Then...this is true...a human picture forms of Franklin Roosevelt and the NRA eagle. This may be the only Hollywood musical production that has ever featured Roosevelt, a big federal agency and a bevy of sexy Chinese prostitutes. <br /><br />That's entertainment, folks. It's great! <br /><br />Of course, Chester's Prologues get the big contract and Nan gets Chester. The movie is full of juicy clichés that make us smile. Ruby Keeler is so endearing as she earnestly stomps out her taps with her arms flying that you want to help her along. Joan Blondell makes us forget about a lot of Hollywood females who might have been more beautiful but who had a lot less wit and personality. The movie, however, belongs to Cagney, who grabs and shakes it, and to Berkeley, a man for whom too much was never too much.\n"
     ]
    }
   ],
   "source": [
    "LINE_BREAK = '<br />'\n",
    "\n",
    "count = 10\n",
    "for text in train_pos_text:\n",
    "    if LINE_BREAK in text:\n",
    "        print(text)\n",
    "        count -= 1\n",
    "        if count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_line_break_usage(reviews):\n",
    "    count = 0\n",
    "    for i, t in enumerate(reviews):\n",
    "        if LINE_BREAK in t:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "print(count_line_break_usage(train_pos_text),\n",
    "      count_line_break_usage(train_neg_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some many line breaks.. we will need to deal with them.\\\n",
    "We will surround them with spaces and remove spaces inside the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_LINE_BREAK = ' ~br~ '\n",
    "\n",
    "train_pos_text = [text.replace(LINE_BREAK, NEW_LINE_BREAK) for text in train_pos_text]\n",
    "train_neg_text = [text.replace(LINE_BREAK, NEW_LINE_BREAK) for text in train_neg_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(count_line_break_usage(train_pos_text),\n",
    "      count_line_break_usage(train_neg_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(English().vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(reviews):\n",
    "    return [tokenizer(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_tokens = tokenize_reviews(train_pos_text)\n",
    "train_neg_tokens = tokenize_reviews(train_neg_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = list(set([\n",
    "    tok.text\n",
    "    for review in train_pos_tokens + train_neg_tokens\n",
    "    for tok in review\n",
    "]))\n",
    "\n",
    "tok2id = { tok: i for i, tok in enumerate(all_tokens) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokenized_reviews):\n",
    "    return Counter([\n",
    "        tok2id[tok.text]\n",
    "        for review in tokenized_reviews\n",
    "        for tok in review\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_token_counter = count_tokens(train_pos_tokens)\n",
    "neg_token_counter = count_tokens(train_neg_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the usage of some meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meaningful_words = ['best', 'love', 'fantastic', 'terrific', 'horrible', 'worst', 'bad']\n",
    "\n",
    "# for word in meaningful_words:\n",
    "#     pos = pos_token_counter[tok2id[word]]\n",
    "#     neg = neg_token_counter[tok2id[word]]\n",
    "#     print(f\"{word}: {pos=}, {neg=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "\n",
    "        # Special characters\n",
    "        self.start = '<'\n",
    "        self.end = '>'\n",
    "\n",
    "        # Actual pairs of samples\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "        # Construct the actual dataset\n",
    "        self.__create_dataset(texts)\n",
    "\n",
    "    def __create_dataset(self, texts):        \n",
    "\n",
    "        # Construct token hierarchy\n",
    "        tokens = [\n",
    "            [self.start] + word_tokenize(s) + [self.end] \n",
    "            for text in texts\n",
    "            for s in sent_tokenize(text)\n",
    "        ]\n",
    "\n",
    "        # Build dictionary\n",
    "        self.id2word = []\n",
    "        for s in tokens:\n",
    "            self.id2word = list(set(self.id2word + s))\n",
    "\n",
    "        self.word2id = {word: i for i, word in enumerate(self.id2word)}\n",
    "        self.id2word = {i: word for i, word in enumerate(self.id2word)}\n",
    "\n",
    "        # For every sentence\n",
    "        for s in tokens:\n",
    "            \n",
    "            # For every target word\n",
    "            for i, wi in enumerate(s[1:], start=1):\n",
    "\n",
    "                # word ids\n",
    "                self.X.append([self.word2id[w] for w in s[:i]])\n",
    "\n",
    "                # target word id\n",
    "                self.y.append(self.word2id[s[i]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = train_pos_text + train_neg_text + train_unlabeled_text\n",
    "imdb_train_dataset = LanguageModelDataset(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8147\n",
      "[365] 1355\n",
      "[365, 1355] 343\n",
      "[365, 1355, 343] 658\n",
      "[365, 1355, 343, 658] 69\n",
      "[365, 1355, 343, 658, 69] 1567\n",
      "[365, 1355, 343, 658, 69, 1567] 53\n",
      "[365] 1355\n",
      "[365, 1355] 320\n",
      "[365, 1355, 320] 320\n",
      "[365, 1355, 320, 320] 780\n",
      "365\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(imdb_train_dataset))\n",
    "for i in range(10):\n",
    "    x, y = imdb_train_dataset[i]\n",
    "    print(x, y)\n",
    "\n",
    "print(imdb_train_dataset.word2id['<'])\n",
    "print(imdb_train_dataset.word2id['>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch):\n",
    "    features = pack_sequence([torch.tensor(sample[0], dtype=torch.long) for sample in batch], enforce_sorted=False)\n",
    "    labels = torch.tensor([sample[1] for sample in batch], dtype=torch.long)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_dataloader = DataLoader(imdb_train_dataset, batch_size=64, shuffle=True, collate_fn=my_collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model Definition & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dim, cell_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.cell_dim = cell_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Net\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_dim)\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=cell_dim, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(in_features=cell_dim, out_features=min(cell_dim * 2, (cell_dim + vocab_size) // 2))\n",
    "        self.fc2 = nn.Linear(in_features=min(cell_dim * 2, (cell_dim + vocab_size) // 2), out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Select embeddings\n",
    "        unpacked_x = unpack_sequence(x)\n",
    "        batch_size = len(unpacked_x)\n",
    "        embeddings = [self.embedding(sample) for sample in unpacked_x]\n",
    "        packed_embeddings = pack_sequence(embeddings, enforce_sorted=False)\n",
    "\n",
    "        # Initialize hidden & cell states for LSTM\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.cell_dim)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.cell_dim)\n",
    "\n",
    "        # Forward LSTM\n",
    "        output, (h_n, c_n) = self.lstm(packed_embeddings, (h_0, c_0))\n",
    "\n",
    "        # Forward Fully-Connected\n",
    "        pred = F.relu(self.fc1(c_n[0]))\n",
    "        pred = self.fc2(pred)\n",
    "\n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_lm = LanguageModel(vocab_size=len(imdb_train_dataset.id2word), input_dim=100, cell_dim=100, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdb_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch 1 ---------------\n",
      "  batch 32/128 loss: 7.221545219421387\n",
      "  batch 64/128 loss: 6.406396195292473\n",
      "  batch 96/128 loss: 6.209994316101074\n",
      "  batch 128/128 loss: 6.228965267539024\n",
      "--------------- epoch 2 ---------------\n",
      "  batch 32/128 loss: 5.587084472179413\n",
      "  batch 64/128 loss: 5.532309025526047\n",
      "  batch 96/128 loss: 5.49144184589386\n",
      "  batch 128/128 loss: 5.470043316483498\n",
      "--------------- epoch 3 ---------------\n",
      "  batch 32/128 loss: 5.049678564071655\n",
      "  batch 64/128 loss: 4.969946637749672\n",
      "  batch 96/128 loss: 5.050805062055588\n",
      "  batch 128/128 loss: 4.99445204436779\n",
      "--------------- epoch 4 ---------------\n",
      "  batch 32/128 loss: 4.5196719616651535\n",
      "  batch 64/128 loss: 4.519476473331451\n",
      "  batch 96/128 loss: 4.482130333781242\n",
      "  batch 128/128 loss: 4.374301441013813\n",
      "--------------- epoch 5 ---------------\n",
      "  batch 32/128 loss: 3.842666707932949\n",
      "  batch 64/128 loss: 3.8551588356494904\n",
      "  batch 96/128 loss: 3.8436590805649757\n",
      "  batch 128/128 loss: 3.7223614379763603\n",
      "--------------- epoch 6 ---------------\n",
      "  batch 32/128 loss: 3.0362119525671005\n",
      "  batch 64/128 loss: 3.033303439617157\n",
      "  batch 96/128 loss: 3.0356955379247665\n",
      "  batch 128/128 loss: 2.959166258573532\n",
      "--------------- epoch 7 ---------------\n",
      "  batch 32/128 loss: 2.2030128985643387\n",
      "  batch 64/128 loss: 2.3160301595926285\n",
      "  batch 96/128 loss: 2.3419688157737255\n",
      "  batch 128/128 loss: 2.342115305364132\n",
      "--------------- epoch 8 ---------------\n",
      "  batch 32/128 loss: 1.6688081994652748\n",
      "  batch 64/128 loss: 1.8084718845784664\n",
      "  batch 96/128 loss: 1.8547349274158478\n",
      "  batch 128/128 loss: 1.8605847023427486\n",
      "--------------- epoch 9 ---------------\n",
      "  batch 32/128 loss: 1.3730118945240974\n",
      "  batch 64/128 loss: 1.3820650056004524\n",
      "  batch 96/128 loss: 1.4756781719624996\n",
      "  batch 128/128 loss: 1.5264671593904495\n",
      "--------------- epoch 10 ---------------\n",
      "  batch 32/128 loss: 1.0588199030607939\n",
      "  batch 64/128 loss: 1.1483429670333862\n",
      "  batch 96/128 loss: 1.184183944016695\n",
      "  batch 128/128 loss: 1.2434020191431046\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(imdb_lm.parameters())\n",
    "\n",
    "imdb_lm.train()\n",
    "num_epochs = 10\n",
    "verbose_freq = 32\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    print(f\"--------------- epoch {e} ---------------\")\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, (X, y) in enumerate(imdb_train_dataloader, start=1):\n",
    "\n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        predictions = imdb_lm(X)\n",
    "\n",
    "        # Backward\n",
    "        loss = loss_fn(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % verbose_freq == 0:\n",
    "            last_loss = running_loss / verbose_freq\n",
    "            running_loss = 0.\n",
    "            print(f'  batch {i}/{len(imdb_train_dataloader)} loss: {last_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Plot : Sixteen-year-old Alice Palmer and Alessi Boni ( cute , art .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(gpt, start, end, id2word, word2id):\n",
    "    generated = []\n",
    "\n",
    "    tok = word2id[start]\n",
    "    word = start\n",
    "    while True:\n",
    "\n",
    "        # generate\n",
    "        t = torch.tensor([tok])\n",
    "        p = pack_sequence([t])\n",
    "        tok = torch.argmax(gpt(p)).item()\n",
    "        word = id2word[tok]\n",
    "        \n",
    "        if word == end:\n",
    "            break\n",
    "\n",
    "        generated.append(word)\n",
    "    \n",
    "    return ' '.join(generated)\n",
    "\n",
    "generate(imdb_lm, \n",
    "         imdb_train_dataset.start,\n",
    "         imdb_train_dataset.end,\n",
    "         imdb_train_dataset.id2word,\n",
    "         imdb_train_dataset.word2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Text Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset & Dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3-A Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3-B End-to-End"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4-A Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4-B End-to-End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
